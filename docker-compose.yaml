services:
  backend:
    image: nilsherzig/llocalsearch-backend:latest
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - CHROMA_DB_URL=${CHROMA_DB_URL:-http://chromadb:8000}
      - SEARXNG_DOMAIN=${SEARXNG_DOMAIN:-http://searxng:8080}
      - EMBEDDINGS_MODEL_NAME=${EMBEDDINGS_MODEL_NAME:-nomic-embed-text:v1.5}
    networks:
      - llm_network

  frontend:
    depends_on:
      - backend
    image: nilsherzig/llocalsearch-frontend:latest
    ports:
      - '3000:4173'
    networks:
      - llm_network

  chromadb:
    image: chromadb/chroma
    networks:
      - llm_network

  redis:
    image: docker.io/library/redis:alpine
    command: redis-server --save 30 1 --loglevel warning
    networks:
      - searxng
    volumes:
      - redis-data:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE

  searxng:
    image: docker.io/searxng/searxng:latest
    networks:
      - searxng
      - llm_network
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: 'json-file'
      options:
        max-size: '1m'
        max-file: '1'

  #####################################
  # the following is an example how you could run
  # ollama inside the "llm_network_dev" network
  # this allows the backend to connect to
  # ollama using the following url
  #
  # http://ollama:11434 as OLLAMA_HOST (default value)
  #
  # ðŸ”´ Before running this, make sure you have
  # read this: https://hub.docker.com/r/ollama/ollama
  #####################################

  # if you have an AMD GPU
  # ollama:
  #   image: ollama/ollama:rocm
  #   # this overwrite is only needed if your amd gpu is not officially supported by rocm
  #   # environment:
  #   #   - HSA_OVERRIDE_GFX_VERSION=10.3.0
  #   devices:
  #     - /dev/kfd
  #     - /dev/dri
  #   volumes:
  #     - ollama:/root/.ollama
  #   networks:
  #     - llm_network_dev

  # if you have an NVIDIA GPU
  # ollama:
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities:
  #               - gpu
  #   volumes:
  #     - ollama:/root/.ollama
  #   ports:
  #     - 11434:11434
  #   image: ollama/ollama
  #   networks:
  #     - llm_network_dev

networks:
  llm_network:
    driver: bridge
  searxng:
    ipam:
      driver: default

volumes:
  redis-data:
  ollama:
    external: true
    name: ollama
