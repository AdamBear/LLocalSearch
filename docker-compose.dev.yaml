services:
  backend-dev:
    volumes:
      - ./backend/:/app/
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    environment:
      ##### change these
      # the name of the model on YOUR Ollama server which will be used
      - OLLAMA_MODEL_NAME=knoopx/hermes-2-pro-mistral:7b-q8_0
      # the ip / url of YOUR Ollama server
      - OLLAMA_HOST=http://192.168.0.109:11434

      ##### no need to change these
      - CHROMA_DB_URL=http://chromadb:8000
      - SEARXNG_DOMAIN=http://searxng:8080
      # the maximum amount of iterations the agent will run to find your answer
      - MAX_ITERATIONS=30
    networks:
      - llm_network_dev

  frontend-dev:
    depends_on:
      - backend-dev
    build:
      context: .
      dockerfile: Dockerfile.dev
    volumes:
      - ./:/app/
    environment:
      - BACKEND_URL=http://backend:8080
    ports:
      - '3000:5173'
    networks:
      - llm_network_dev

  chromadb:
    image: chromadb/chroma
    networks:
      - llm_network_dev
    # attach: false
    # logging:
    #   driver: none

  redis:
    container_name: redis
    image: docker.io/library/redis:alpine
    command: redis-server --save 30 1 --loglevel warning
    networks:
      - searxng
    volumes:
      - redis-data:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    networks:
      - searxng
      - llm_network_dev
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: 'json-file'
      options:
        max-size: '1m'
        max-file: '1'

networks:
  llm_network_dev:
    driver: bridge
  searxng:
    ipam:
      driver: default

volumes:
  redis-data:
